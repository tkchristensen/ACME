{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Work\n",
    "\n",
    "### Problem 1(a)\n",
    "For the mean squared error loss function $L = \\frac{1}{2}(y - f(x))^2$, prove that $g(x) = -(y - f(x))$ and calculate $h(x)$.\n",
    "\n",
    "##### Solution\n",
    "We are given that,\n",
    "\n",
    "$$g_i = \\partial_{\\hat{y_i}^{(t-1)}} L$$\n",
    "$$h_i = \\partial_{\\hat{y_i}^{(t-1)}}^2 L$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$ \\hat{y_i}^{(t)} = \\sum_{k=1}^{t} f_k(x_i) $$\n",
    "\n",
    "For notational consistency between the problem statement and the xgboost documentation, set $f(x) = \\hat{y}^{T}$ to be the sum of all the trees up to a given iteration $T$. \n",
    "\n",
    "\\begin{align*}\n",
    "g_i & = \\partial_{f(x_i)} \\frac{1}{2}(y_i - f(x_i))^2 \\\\\n",
    " & = (y_i - f(x_i))(-1) \\\\\n",
    " & = -(y_i - f(x_i)) .\n",
    "\\end{align*}\n",
    "\n",
    "So $g(x) = -(y - f(x))$.\n",
    "\n",
    "\\begin{align*}\n",
    "h_i & = \\partial_{f(x_i)}^2 \\frac{1}{2}(y_i - f(x_i))^2 \\\\\n",
    " & = -\\partial_{f(x_i)} (y_i - f(x_i)) \\\\\n",
    " & = 1\n",
    "\\end{align*}\n",
    "\n",
    "So $h(x) = 1$.\n",
    "\n",
    "### Problem 1(b)\n",
    "Assuming that the function $f_0(x) = 0$ for all $x$ and assuming that the data points $(x_i, y_i)$ all have $y_i$ equal to either $+1$ or $-1$, write out the corresponding functions $G$ and $H$ for the next tree, $f_1$. For a given proposed split of a leaf into leaves L and R, let\n",
    "\n",
    "$l_p$ be the number of data points with $y_i=+1$ in the L leaf,\n",
    "\n",
    "$l_m$ be the number of data points with $y_i=-1$ in the L leaf,\n",
    "\n",
    "$r_p$ be the number of data points with $y_i=+1$ in the R leaf,\n",
    "\n",
    "$r_m$ be the number of data points with $y_i=-1$ in the R leaf.\n",
    "\n",
    "Show that the gain function used for splitting a tree in xgboost in this special case is \n",
    "\n",
    "$$Gain = \\frac{(l_p - l_m)^2}{2(l_p + l_m + \\lambda)}  + \\frac{(r_p - r_m)^2}{2(r_p + r_m + \\lambda)} - \\frac{(l_p + r_p - l_m - r_m)^2}{2(l_p + r_p + l_m + r_m + \\lambda)} - \\gamma$$\n",
    "\n",
    "\n",
    "##### Solution\n",
    "\n",
    "In the xgboost documentation, we are given that,\n",
    "\n",
    "$$Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma$$\n",
    "\n",
    "This problem, therefore, reduces to showing that,\n",
    "\\begin{align*}\n",
    "G_L & = \\pm (l_p - l_m) \\\\\n",
    "G_R & = \\pm (r_p - r_m) \\\\\n",
    "H_L & = l_p + l_m \\\\\n",
    "H_R & = r_p + r_m \n",
    "\\end{align*}\n",
    "\n",
    "By definition given in the documentation,\n",
    "\\begin{align*}\n",
    "G_L & = \\sum_{i \\in I_L} g_i \\\\\n",
    " & = \\sum_{y_i = +1} g_i + \\sum_{y_i = -1} g_i \\\\\n",
    " & = \\sum_{y_i = +1} -1 + f_0(x_i) + \\sum_{y_i = -1} 1 + f_0(x_i) \\\\\n",
    " & = 2\\sum_{i} f(x_i) + \\sum_{y_i = +1} -1 + \\sum_{y_i = -1} 1 \\\\\n",
    " & = \\sum_{y_i = +1} -1 + \\sum_{y_i = -1} 1 \\\\\n",
    " & = -l_p + l_m \\\\\n",
    " & = -(l_p - l_m)\n",
    "\\end{align*}\n",
    "\n",
    "$G_R = -(r_p - r_m)$ follows similarly.\n",
    "\n",
    "By defintion given in the documentation, \n",
    "\\begin{align*}\n",
    "H_L & = \\sum{i \\in I_L} h_i \\\\\n",
    " & = \\sum_{y_i = +1} h_i + \\sum_{y_i = -1} h_i \\\\\n",
    " & = \\sum_{y_i = +1} 1 + \\sum_{y_i = -1} 1 \\\\\n",
    " & = l_p + l_m\n",
    "\\end{align*}\n",
    "\n",
    "$H_R = r_p + r_m$ follows similarly.   QED\n",
    "\n",
    "### Problem 2\n",
    "Do the same computations for the adaboost loss function $L(y,f(x)) = e^{-yf(x)}.$ \n",
    "\n",
    "##### Solution.\n",
    "For brevity, I will not reintroduce notation and I will not be as thorough in presenting all my work.\n",
    "$$L = e^{-yf(x)}$$\n",
    "Therefore, \n",
    "$$g = -ye^{-yf(x)}$$\n",
    "$$h = y^2e^{-yf(x)}$$.\n",
    "\n",
    "\\begin{align*}\n",
    "G_L & = \\sum_{i \\in I_L} g_i \\\\\n",
    " & = \\sum_{y_i = +1} g_i + \\sum_{y_i = -1} g_i \\\\\n",
    " & = \\sum_{y_i = +1} -e^{-f_0(x_i)} + \\sum_{y_i = -1} e^{f_0(x_i)} \\\\\n",
    " & = \\sum_{y_i = +1} -1 + \\sum_{y_i = -1} 1 \\\\\n",
    " & = -l_p + l_m \\\\\n",
    " & = -(l_p - l_m)\n",
    "\\end{align*}\n",
    "\n",
    "$G_R = -(r_p - r_m)$ follows similarly.\n",
    "\n",
    "\\begin{align*}\n",
    "H_L & = \\sum{i \\in I_L} h_i \\\\\n",
    " & = \\sum_{y_i = +1} h_i + \\sum_{y_i = -1} h_i \\\\\n",
    " & = \\sum_{y_i = +1} y_i^2e^{-y_if_0(x)} + \\sum_{y_i = -1} y_i^2e^{-y_if(x)} \\\\\n",
    " & = \\sum_{y_i = +1} (1)^2e^{-f_0(x)} + \\sum_{y_i = -1} (-1)^2e^{f_0(x)} \\\\\n",
    " & = \\sum_{y_i = +1} 1 + \\sum_{y_i = -1} 1 \\\\\n",
    " & = l_p + l_m\n",
    "\\end{align*}\n",
    "\n",
    "$H_R = r_p + r_m$ follows similarly.   QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing the XGBoost Algorithm\n",
    "__NOTE:__ due to the grid searching used to tune the parameters, it is not in your best interest to rerun this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T22:35:40.122822",
     "start_time": "2017-02-20T22:35:40.117507"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:40.631330",
     "start_time": "2017-02-20T18:13:40.616666"
    }
   },
   "outputs": [],
   "source": [
    "breast_cancer = load_breast_cancer()\n",
    "data = pd.DataFrame(breast_cancer[\"data\"], columns=breast_cancer[\"feature_names\"])\n",
    "target = breast_cancer[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:41.006086",
     "start_time": "2017-02-20T18:13:40.957420"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension           ...             worst radius  \\\n",
       "0                 0.07871           ...                    25.38   \n",
       "1                 0.05667           ...                    24.99   \n",
       "2                 0.05999           ...                    23.57   \n",
       "3                 0.09744           ...                    14.91   \n",
       "4                 0.05883           ...                    22.54   \n",
       "\n",
       "   worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0          17.33           184.60      2019.0            0.1622   \n",
       "1          23.41           158.80      1956.0            0.1238   \n",
       "2          25.53           152.50      1709.0            0.1444   \n",
       "3          26.50            98.87       567.7            0.2098   \n",
       "4          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   worst fractal dimension  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:42.368001",
     "start_time": "2017-02-20T18:13:42.355771"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(tree, features, target, iters=20):\n",
    "    \"\"\"\n",
    "    Test a decision tree model based on inputed data and target.\n",
    "    Print the average misclassification rate, variance, and \n",
    "    average training time of the model.\n",
    "    \n",
    "    Inputs:\n",
    "        tree - sklearn DecisionTreeClassifier object\n",
    "        features - features of the data\n",
    "        target - target values for the data\n",
    "        iters (optional) - number of iterations to test model. \n",
    "            Defaults to 50.\n",
    "    \"\"\"\n",
    "    acc = []\n",
    "    t = []\n",
    "    for i in xrange(iters):\n",
    "        # create a new train-test split each iteration\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(features, target, train_size=.7)\n",
    "        \n",
    "        # fit and time\n",
    "        before = time.time()\n",
    "        tree.fit(Xtrain, ytrain)\n",
    "        after = time.time()\n",
    "        t.append(after-before)\n",
    "        \n",
    "        # calculate and store misclassification rate\n",
    "        acc.append(1 - (tree.predict(Xtest) == ytest).mean())\n",
    "    print \"Average misclassification rate:\", np.mean(acc)\n",
    "    print \"Variance of misclassification rate:\", np.var(acc)\n",
    "    print \"Average training time:\", np.mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    \"\"\"\n",
    "    Print the 'n_top' best model parameters and performance.\n",
    "    \"\"\"\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning xgboost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:45.433659",
     "start_time": "2017-02-20T18:13:44.059586"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.0388888888889\n",
      "Variance of misclassification rate: 0.000161673677371\n",
      "Average training time: 0.0661438107491\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "test_model(xgb, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:47.003008",
     "start_time": "2017-02-20T18:13:45.435635"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.0397660818713\n",
      "Variance of misclassification rate: 0.000169624841832\n",
      "Average training time: 0.0757920622826\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(max_depth=4)\n",
    "test_model(xgb1, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:48.431338",
     "start_time": "2017-02-20T18:13:47.004786"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.0362573099415\n",
      "Variance of misclassification rate: 0.00016620498615\n",
      "Average training time: 0.0688552618027\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(min_child_weight=.8)\n",
    "test_model(xgb2, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:49.505906",
     "start_time": "2017-02-20T18:13:48.433296"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.040350877193\n",
      "Variance of misclassification rate: 0.000280086180363\n",
      "Average training time: 0.05118714571\n"
     ]
    }
   ],
   "source": [
    "xgb3 = XGBClassifier(learning_rate=.2)\n",
    "test_model(xgb3, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:13:51.038408",
     "start_time": "2017-02-20T18:13:49.507562"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.0377192982456\n",
      "Variance of misclassification rate: 0.000295732020109\n",
      "Average training time: 0.0739785313606\n"
     ]
    }
   ],
   "source": [
    "xgb4 = XGBClassifier(n_estimators=120)\n",
    "test_model(xgb4, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:14:10.940561",
     "start_time": "2017-02-20T18:14:10.365086"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.030701754386\n",
      "Variance of misclassification rate: 0.000156885879416\n",
      "Average training time: 0.0261357426643\n"
     ]
    }
   ],
   "source": [
    "xgb5 = XGBClassifier(subsample=.7, colsample_bytree=.3)\n",
    "test_model(xgb5, data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Parameters\n",
    "For my 6th xgboost model, I will use a step-by-step parameter grid search to find the best model I can. \n",
    "\n",
    "I will determine the optimal parameter values based on the subset of parameters I'm considering at each step. At each step, I will add a few more parameters and repeat the process until I have searched a variety of values for all parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning_rate and n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:25:44.734678",
     "start_time": "2017-02-20T18:25:44.726598"
    }
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'learning_rate': np.arange(0.05,0.25,.025),\n",
    "    'silent': [1.0],\n",
    "    'n_estimators': np.arange(100,200,10)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid1)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T18:29:23.436332",
     "start_time": "2017-02-20T18:29:23.428185"
    }
   },
   "outputs": [],
   "source": [
    "param_grid2 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [3,4,5,6], #4\n",
    "    'min_child_weight': np.arange(.5,1.5,.1), #1\n",
    "    'learning_rate': [.2],\n",
    "    'silent': [1.0], \n",
    "    'n_estimators': [120]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid2)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T20:13:46.153340",
     "start_time": "2017-02-20T20:13:43.656422"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.974 (std: 0.004)\n",
      "Parameters: {'silent': 1.0, 'learning_rate': 0.2, 'min_child_weight': 1.3, 'n_estimators': 120, 'objective': 'binary:logistic', 'max_depth': 4, 'gamma': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid3 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1.3],\n",
    "    'learning_rate': [.2],\n",
    "    'silent': [1.0], \n",
    "    'n_estimators': [120],\n",
    "    'gamma':np.arange(0,1,.1)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid3)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T20:14:19.965300",
     "start_time": "2017-02-20T20:14:11.448523"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.981 (std: 0.010)\n",
      "Parameters: {'colsample_bytree': 0.30000000000000004, 'silent': 1.0, 'learning_rate': 0.2, 'min_child_weight': 1.3, 'n_estimators': 120, 'subsample': 0.90000000000000002, 'objective': 'binary:logistic', 'max_depth': 4, 'gamma': 0}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.981 (std: 0.007)\n",
      "Parameters: {'colsample_bytree': 0.40000000000000002, 'silent': 1.0, 'learning_rate': 0.2, 'min_child_weight': 1.3, 'n_estimators': 120, 'subsample': 0.70000000000000007, 'objective': 'binary:logistic', 'max_depth': 4, 'gamma': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid4 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1.3],\n",
    "    'learning_rate': [.2],\n",
    "    'silent': [1.0], \n",
    "    'n_estimators': [120],\n",
    "    'gamma':[0],\n",
    "    'subsample':np.arange(0.1,1,.1),\n",
    "    'colsample_bytree':np.arange(0.1,1,.1)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid4)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_alpha and reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T19:23:34.126454",
     "start_time": "2017-02-20T19:23:34.117594"
    }
   },
   "outputs": [],
   "source": [
    "param_grid5 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1.3],\n",
    "    'learning_rate': [.2],\n",
    "    'silent': [1.0], \n",
    "    'n_estimators': [120],\n",
    "    'reg_alpha':[10**t for t in [-10, -5, -3, -2, -1, 0]], #0.001\n",
    "    'reg_lambda':np.arange(0,1,.1), #0.8\n",
    "    'gamma':[0],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid5)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T19:29:43.190819",
     "start_time": "2017-02-20T19:29:41.896474"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.981 (std: 0.007)\n",
      "Parameters: {'reg_alpha': 0.001, 'colsample_bytree': 0.4, 'silent': 1.0, 'scale_pos_weight': 1.0, 'learning_rate': 0.2, 'min_child_weight': 1.3, 'n_estimators': 120, 'subsample': 0.7, 'reg_lambda': 0.6, 'objective': 'binary:logistic', 'max_depth': 4, 'gamma': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid6 = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1.3],\n",
    "    'learning_rate': [.2],\n",
    "    'silent': [1.0], \n",
    "    'n_estimators': [120],\n",
    "    'reg_alpha': [0.001], #0.001\n",
    "    'reg_lambda': [0.6], #0.8\n",
    "    'gamma':[0],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.4],\n",
    "    'scale_pos_weight':np.arange(0,1.25,.1)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid=param_grid6)\n",
    "fit = grid_search.fit(data, target)\n",
    "\n",
    "report(grid_search.cv_results_, n_top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T19:33:27.897930",
     "start_time": "2017-02-20T19:33:27.248969"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average misclassification rate: 0.0321637426901\n",
      "Variance of misclassification rate: 0.000100885742622\n",
      "Average training time: 0.0297340989113\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1.3,\n",
    "    'learning_rate': .2,\n",
    "    'silent': 1,\n",
    "    'n_estimators': 120,\n",
    "    'reg_alpha': 0.001,\n",
    "    'reg_lambda': 0.8,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'scale_pos_weight': 1\n",
    "}\n",
    "\n",
    "best_xgb = XGBClassifier(**xgb_params)\n",
    "test_model(best_xgb, data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "I will compare my refined xgboost model to default decision trees, default random forests, and default gradient boosted trees. I will also compare my refined xgboost model to the best models from my previous homework assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T21:27:23.165163",
     "start_time": "2017-02-20T21:26:24.131887"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification Rates:\n",
      "------------------------\n",
      "  Default Tree:\t0.075730994152\n",
      "  Best Tree:\t0.0661988304094\n",
      "\n",
      "  Default RF:\t0.0488888888889\n",
      "  Best RF:\t0.043918128655\n",
      "\n",
      "  Default GB:\t0.0452631578947\n",
      "  Best GB:\t0.036081871345\n",
      "\n",
      "  Default XGB:\t0.0391228070175\n",
      "  Best XGB:\t0.0338011695906\n",
      "\n",
      "Training Times:\n",
      "---------------\n",
      "  Tree Training Time:\t0.000704345703125\n",
      "  RF Training Time:\t0.199412851334\n",
      "  GB Training Time:\t0.0591463565826\n",
      "  XGB Training Time:\t0.0310510110855\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "gb_params = {'loss': 'exponential', 'learning_rate': 0.5, \n",
    "               'n_estimators': 100, 'min_samples_split': 6, \n",
    "               'max_features': 15, 'max_depth': 2}\n",
    "\n",
    "xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': 4,\n",
    "        'min_child_weight': 1.3,\n",
    "        'learning_rate': .2,\n",
    "        'silent': 1,\n",
    "        'n_estimators': 120,\n",
    "        'reg_alpha': 0.001,\n",
    "        'reg_lambda': 0.8,\n",
    "        'gamma': 0,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.4,\n",
    "        'scale_pos_weight': 1\n",
    "    }\n",
    "\n",
    "best_tree = DecisionTreeClassifier(criterion='entropy', splitter=\"random\", min_samples_leaf=5)\n",
    "best_rf = RandomForestClassifier(n_estimators=50, criterion=\"entropy\", min_samples_split=4)\n",
    "best_gb = GradientBoostingClassifier(**gb_params)\n",
    "best_xgb = XGBClassifier(**xgb_params)\n",
    "\n",
    "tree_times = []\n",
    "rf_times = []\n",
    "gb_times = []\n",
    "xgb_times = []\n",
    "best_tree_times = []\n",
    "best_rf_times = []\n",
    "best_gb_times = []\n",
    "best_xgb_times = []\n",
    "\n",
    "tree_acc = []\n",
    "rf_acc = []\n",
    "gb_acc = []\n",
    "xgb_acc = []\n",
    "best_tree_acc = []\n",
    "best_rf_acc = []\n",
    "best_gb_acc = []\n",
    "best_xgb_acc = []\n",
    "\n",
    "for i in xrange(100):\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(data, target, train_size=.7)\n",
    "    \n",
    "    before = time.time()\n",
    "    tree.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    tree_times.append(after - before)\n",
    "    tree_acc.append(1 - (tree.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    rf.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    rf_times.append(after - before)\n",
    "    rf_acc.append(1 - (rf.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    gb.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    gb_times.append(after - before)\n",
    "    gb_acc.append(1 - (gb.predict(Xtest) == ytest).mean()) \n",
    "\n",
    "    before = time.time()\n",
    "    xgb.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    xgb_times.append(after - before)\n",
    "    xgb_acc.append(1 - (xgb.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    best_tree.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    best_tree_times.append(after - before)\n",
    "    best_tree_acc.append(1 - (best_tree.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    best_rf.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    best_rf_times.append(after - before)\n",
    "    best_rf_acc.append(1 - (best_rf.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    best_gb.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    best_gb_times.append(after - before)\n",
    "    best_gb_acc.append(1 - (best_gb.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "    before = time.time()\n",
    "    best_xgb.fit(Xtrain, ytrain)\n",
    "    after = time.time()\n",
    "    best_xgb_times.append(after - before)\n",
    "    best_xgb_acc.append(1 - (best_xgb.predict(Xtest) == ytest).mean()) \n",
    "    \n",
    "print \"Misclassification Rates:\"\n",
    "print \"------------------------\"\n",
    "print \"  Default Tree:\\t\", np.average(tree_acc)\n",
    "print \"  Best Tree:\\t\", np.average(best_tree_acc)\n",
    "print \"\"\n",
    "print \"  Default RF:\\t\", np.average(rf_acc)\n",
    "print \"  Best RF:\\t\", np.average(best_rf_acc)\n",
    "print \"\"\n",
    "print \"  Default GB:\\t\", np.average(gb_acc)\n",
    "print \"  Best GB:\\t\", np.average(best_gb_acc)\n",
    "print \"\"\n",
    "print \"  Default XGB:\\t\", np.average(xgb_acc)\n",
    "print \"  Best XGB:\\t\", np.average(best_xgb_acc)\n",
    "print \"\"\n",
    "print \"Training Times:\"\n",
    "print \"---------------\"\n",
    "print \"  Tree Training Time:\\t\", np.average(best_tree_times)\n",
    "print \"  RF Training Time:\\t\", np.average(best_rf_times)\n",
    "print \"  GB Training Time:\\t\", np.average(best_gb_times)\n",
    "print \"  XGB Training Time:\\t\", np.average(best_xgb_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the refined models outperformed their default counterparts. \n",
    "\n",
    "I was surprised to see how fast xgboost was in comparison to regular gradient boosting--nearly twice as fast. This was especially surprising since the n_estimators=100, max_depth=2 on the gradient boosted trees whereas n_estimators=120, max_depth=4 on xgboost. I am unsure if this is a difference in the quality of the implemations or if it due to the algorithms itself. As another comparison, unsurprisingly, decision trees were by far the fastest.\n",
    "\n",
    "It was interesting to see that xgboost did not experience a major improvement over gradient boosted trees. I think this is a testament to how similar they are. The extra care given to regularization is what gives xgboost the edge."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
